{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6990291416677233791\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 332398592\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 14078108752158159514\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brats17ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Survival</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brats17_TCIA_167_1</td>\n",
       "      <td>74.907</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brats17_TCIA_242_1</td>\n",
       "      <td>66.479</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brats17_TCIA_319_1</td>\n",
       "      <td>64.860</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brats17_TCIA_469_1</td>\n",
       "      <td>63.899</td>\n",
       "      <td>519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brats17_TCIA_218_1</td>\n",
       "      <td>57.345</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Brats17ID     Age  Survival\n",
       "0  Brats17_TCIA_167_1  74.907       153\n",
       "1  Brats17_TCIA_242_1  66.479       147\n",
       "2  Brats17_TCIA_319_1  64.860       254\n",
       "3  Brats17_TCIA_469_1  63.899       519\n",
       "4  Brats17_TCIA_218_1  57.345       346"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv3D, MaxPooling3D\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 240, 240\n",
    "TUMOR_IMAGES = 155\n",
    "AGE_CLASSES = 100\n",
    "df = pd.read_csv(\"data/survival_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('data/tumors_nz.npy')\n",
    "X = X[:, 14:143, :, :, :] # All images from 0 to 15 and 144 to the end are totally black\n",
    "Y = df['Survival']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSGqMgqTGKEhqjIKkxihIav4PNTYREZLgOisAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3552839490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[100, 77, 2, :, :])\n",
    "np.unique(X[100, 77, 2, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_table = np.eye(AGE_CLASSES)\n",
    "ages_ohe = np.array([enc_table[int(round(x))] for x in df['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "163/163 [==============================] - 19s - loss: 359628.3121 - main_output_loss: 299691.5955 - aux_output_loss: 299683.5771    \n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 9s - loss: 359497.8582 - main_output_loss: 299581.5461 - aux_output_loss: 299581.5461     \n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 9s - loss: 359497.8558 - main_output_loss: 299581.5460 - aux_output_loss: 299581.5460     \n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 9s - loss: 359497.8516 - main_output_loss: 299581.5461 - aux_output_loss: 299581.5461     \n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 9s - loss: 359497.8480 - main_output_loss: 299581.5461 - aux_output_loss: 299581.5461     \n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 9s - loss: 359497.8568 - main_output_loss: 299581.5464 - aux_output_loss: 299581.5464     \n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 9s - loss: 359497.8520 - main_output_loss: 299581.5462 - aux_output_loss: 299581.5462     \n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 9s - loss: 359497.8528 - main_output_loss: 299581.5456 - aux_output_loss: 299581.5456     \n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 9s - loss: 359497.8626 - main_output_loss: 299581.5460 - aux_output_loss: 299581.5460     \n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 9s - loss: 359497.8547 - main_output_loss: 299581.5466 - aux_output_loss: 299581.5466     \n",
      "Epoch 11/50\n",
      " 32/163 [====>.........................] - ETA: 7s - loss: 335881.0000 - main_output_loss: 279900.8438 - aux_output_loss: 279900.8438"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    main_input = Input(shape=X.shape[1:], dtype='float32', name='main_input')\n",
    "    x = Conv3D(16, (3, 3, 3), padding='same', activation='relu')(main_input)\n",
    "    x = Conv3D(16, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling3D(pool_size=(2, 1, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv3D(32, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv3D(32, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling3D(pool_size=(2, 1, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    cnn_out = Dense(64, activation='relu')(x)\n",
    "    auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(cnn_out)\n",
    "    \n",
    "    auxiliary_input = Input(shape=(AGE_CLASSES,), name='aux_input', dtype='float32')\n",
    "    x = keras.layers.concatenate([cnn_out, auxiliary_input])\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    main_output = Dense(1, activation='sigmoid', name='main_output')(x)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])\n",
    "    \n",
    "    # RMSprop uses:\n",
    "    # - Momentum taking knowledge from previous steps into account about where\n",
    "    #   we should be heading (prevents gradient descent to oscillate)\n",
    "    # - Uses recent gradients to adjust alpha\n",
    "    #   (when the gradient is very large, alpha is reduced and vice-versa)\n",
    "    # Later we should test if AdaDelta or Adam are improving our results (quite similar to RMSprop)\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss={'main_output': 'mean_squared_error', 'aux_output': 'mean_squared_error'},\n",
    "              loss_weights={'main_output': 1., 'aux_output': 0.2})\n",
    "\n",
    "    # And trained it via:\n",
    "    model.fit({'main_input': X, 'aux_input': ages_ohe},\n",
    "              {'main_output': Y, 'aux_output': Y},\n",
    "              epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 163 patients with tumor images of sizes 240x240 of which we have 155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start\n",
    "Implementing a concatenated model as shown in the Docs: https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models\n",
    "The model uses a small common CNN for processing one patient (155 slices x 128 width x 128 height) and just inserts the age at a certain point. With this simple model we get really bad results (main_output_loss: 263381.5156 [not getting better]).\n",
    "\n",
    "\n",
    "### Possible improvements\n",
    "1. Conv3D (in CNN) which gets a 4D input - therefore add the tumor region as a dimension (transform input image into three images containing each tumor region seperately)\n",
    "2. One hot encoder (in sklearn or implement manually)\n",
    "  [ I tried Keras Lambda layer but after fixing the shapes keras told me that I'm not allowed to concatenate with a non input layer ]\n",
    "3. Group ages into few classes changing the regression task into a classification task (with softmax).\n",
    "4. Evaluate if LSTM is useful for this task (before concatenating the two branches or as a concatenator?)\n",
    "5. Evaluate our optimizer (RMSprp vs Adam vs AdaDelta)\n",
    "6. I read full batch learning is the best way for training on few training data - check if this is helpful.\n",
    "7. Use for a patient nx, ny and nz at the same time\n",
    "8. Data augmentation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
