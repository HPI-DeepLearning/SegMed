{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15288776373839293115\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 242221056\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 1558136142775106907\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brats17ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Survival</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brats17_TCIA_167_1</td>\n",
       "      <td>74.907</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brats17_TCIA_242_1</td>\n",
       "      <td>66.479</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brats17_TCIA_319_1</td>\n",
       "      <td>64.860</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brats17_TCIA_469_1</td>\n",
       "      <td>63.899</td>\n",
       "      <td>519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brats17_TCIA_218_1</td>\n",
       "      <td>57.345</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Brats17ID     Age  Survival\n",
       "0  Brats17_TCIA_167_1  74.907       153\n",
       "1  Brats17_TCIA_242_1  66.479       147\n",
       "2  Brats17_TCIA_319_1  64.860       254\n",
       "3  Brats17_TCIA_469_1  63.899       519\n",
       "4  Brats17_TCIA_218_1  57.345       346"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv3D, MaxPooling3D\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 240, 240\n",
    "TUMOR_IMAGES = 155\n",
    "AGE_CLASSES = 100\n",
    "MAX_SURVIVAL = 2000\n",
    "df = pd.read_csv(\"data/survival_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.load('data/tumors_nz.npy')\n",
    "X = X[:, 14:143, :, :, :] # All images from 0 to 15 and 144 to the end are totally black\n",
    "Y = df['Survival']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAC/pJREFUeJzt3U2sXIV5h/HnXxNYJEjg0FqWcQuJvKEbYl1RFihKF02A\njckG0Q1WheQuQEqkduE0i7JtpaQSaorkKCimSqFICcKLfoRYkegGgh0RY6CAk4CwZWxFVAS1UhLg\n7WKOw7zGlzv3Y77a5yeN5syZM3fejO48nHNmbpyqQpIu+J15DyBpsRgFSY1RkNQYBUmNUZDUGAVJ\nzdSikOTWJC8nOZXk4LSeR9LWyjS+p5BkG/AK8CfAaeBZ4E+r6sUtfzJJW2paewo3Aaeq6mdV9Wvg\nUWDflJ5L0ha6bEo/dxfwxtjt08AfrbZxEr9WKU3fL6rqd9faaFpRWFOSA8CBeT2/9P/Q65NsNK0o\nnAF2j92+dlj3W1V1CDgE7ilIi2Ra5xSeBfYkuT7J5cBdwJEpPZekLTSVPYWqejfJfcC/A9uAh6rq\nhWk8l6StNZWPJNc9hIcP0iwcr6qVtTbyG42SGqMgqTEKkhqjIKkxCpIaoyCpMQqSGqMgqTEKkhqj\nIKkxCpIaoyCpMQqSGqMgqTEKkhqjIKkxCpIaoyCpMQqSGqMgqTEKkhqjIKkxCpIaoyCpMQqSGqMg\nqTEKkhqjIKkxCpIaoyCpMQqSGqMgqTEKkhqjIKkxCpIaoyCpuWwzD07yGvAO8B7wblWtJNkO/DNw\nHfAacGdV/dfmxpQ0K1uxp/DHVXVjVa0Mtw8CR6tqD3B0uC1pSUzj8GEfcHhYPgzcMYXnkDQlm41C\nAd9PcjzJgWHdjqo6Oyy/CezY5HNImqFNnVMAbqmqM0l+D3gyyX+O31lVlaQu9cAhIgcudZ+k+dnU\nnkJVnRmuzwOPAzcB55LsBBiuz6/y2ENVtTJ2LkLSAthwFJJ8PMmVF5aBzwMngSPA/mGz/cATmx1S\n0uxs5vBhB/B4kgs/55+q6t+SPAs8luQe4HXgzs2PKWlWUnXJQ/7ZDrHKeQdJW+r4JIfrfqNRUmMU\nJDVGQVJjFCQ1RkFSYxQkNUZBUmMUJDVGQVJjFCQ1RkFSYxQkNUZBUmMUJDVGQVJjFCQ1RkFSYxQk\nNUZBUmMUJDVGQVJjFCQ1RkFSYxQkNUZBUmMUJDVGQVJjFCQ1RkFSYxQkNUZBUmMUJDVGQVJjFCQ1\nRkFSYxQkNUZBUrNmFJI8lOR8kpNj67YneTLJq8P11cP6JHkgyakkJ5LsnebwkrbeJHsK3wZuvWjd\nQeBoVe0Bjg63AW4D9gyXA8CDWzOmpFlZMwpV9RTw1kWr9wGHh+XDwB1j6x+ukaeBq5Ls3KphJU3f\nRs8p7Kiqs8Pym8COYXkX8MbYdqeHdR+S5ECSY0mObXAGSVNw2WZ/QFVVktrA4w4BhwA28nhJ07HR\nPYVzFw4Lhuvzw/ozwO6x7a4d1klaEhuNwhFg/7C8H3hibP3dw6cQNwNvjx1mSFoGVfWRF+AR4Czw\nG0bnCO4BPsnoU4dXgR8A24dtA3wD+CnwPLCy1s8fHldevHiZ+uXYJO/HDG/KufKcgjQTx6tqZa2N\n/EajpMYoSGqMgqTGKEhqjIKkxihIaoyCpMYoSGqMgqTGKEhqjIKkxihIaoyCpMYoSGqMgqTGKEhq\njIKkxihIaoyCpMYoSGqMgqTGKEhqjIKkxihIaoyCpMYoSGqMgqTGKEhqjIKkxihIaoyCpMYoSGqM\ngqTGKEhqjIKkxihIataMQpKHkpxPcnJs3f1JziR5brjcPnbfV5KcSvJyki9Ma3BJ0zHJnsK3gVsv\nsf7vqurG4fIvAEluAO4C/nB4zD8k2bZVw0qavjWjUFVPAW9N+PP2AY9W1a+q6ufAKeCmTcwnacY2\nc07hviQnhsOLq4d1u4A3xrY5PayTtCQ2GoUHgU8DNwJnga+t9wckOZDkWJJjG5xB0hRsKApVda6q\n3quq94Fv8sEhwhlg99im1w7rLvUzDlXVSlWtbGQGLYaqmvcI2mIbikKSnWM3vwhc+GTiCHBXkiuS\nXA/sAX60uRG16DYSBmOyuC5ba4MkjwCfA65Jchr4a+BzSW4ECngN+HOAqnohyWPAi8C7wL1V9d50\nRte8jb+xq4ok63rchetJH6fZyCIUO8n8h9C6rPZ7M8kb/FKPNQwzcXySw3W/0agtVVUTHRokMQQL\nas3DB2mrjcfAMCweo6CpGN9b8I2/XDx8kNQYBa3bek9OL8LJbE3OKGjdNnI4YBiWh1HQzBiG5WAU\nNFOGYfEZBUmNUdBM+fHk4jMKkhqjoJlxL2E5GAVJjVHQhqz3v/ruJSwP//ZBM+HfQiwP9xS0IZv5\nvsGkf16t+TAKmgv3FhaXUdCGbPZN7Z7C4vKcgmbOvYTF5p6CpMYoSGo8fNDMeNiwHNxT0EwYhOVh\nFDR1BmG5GAVJjecUtGmX2hNYzz8jp8XinoI27KPe9AZheRkFbYpv/v97jIKkxihIaoyCpMYoSGqM\ngqTGKEhqjIKkxihIataMQpLdSX6Y5MUkLyT50rB+e5Ink7w6XF89rE+SB5KcSnIiyd5p/4+QtHUm\n2VN4F/iLqroBuBm4N8kNwEHgaFXtAY4OtwFuA/YMlwPAg1s+taSpWTMKVXW2qn48LL8DvATsAvYB\nh4fNDgN3DMv7gIdr5GngqiQ7t3xySVOxrnMKSa4DPgM8A+yoqrPDXW8CO4blXcAbYw87PayTtAQm\n/tPpJJ8Avgt8uap+Of6HMFVVSdb1/9md5ACjwwtJC2SiPYUkH2MUhO9U1feG1ecuHBYM1+eH9WeA\n3WMPv3ZY11TVoapaqaqVjQ4vaetN8ulDgG8BL1XV18fuOgLsH5b3A0+Mrb97+BTiZuDtscMMSQsu\na/1LPUluAf4DeB54f1j9V4zOKzwG/D7wOnBnVb01ROTvgVuB/wH+rKqOrfEc/nNB0vQdn2TPfM0o\nzIJRkGZioij4jUZJjVGQ1BgFSY1RkNQYBUmNUZDUGAVJjVGQ1BgFSY1RkNQYBUmNUZDUGAVJjVGQ\n1BgFSY1RkNQYBUmNUZDUGAVJjVGQ1BgFSY1RkNQYBUmNUZDUGAVJjVGQ1BgFSY1RkNQYBUmNUZDU\nGAVJzWXzHmDwC+C/h+tlcg3OPCvLOPeizfwHk2yUqpr2IBNJcqyqVuY9x3o48+ws49zLODN4+CDp\nIkZBUrNIUTg07wE2wJlnZxnnXsaZF+ecgqTFsEh7CpIWwNyjkOTWJC8nOZXk4LznWU2S15I8n+S5\nJMeGdduTPJnk1eH66gWY86Ek55OcHFt3yTkz8sDw2p9IsneBZr4/yZnh9X4uye1j931lmPnlJF+Y\n08y7k/wwyYtJXkjypWH9Qr/WE6mquV2AbcBPgU8BlwM/AW6Y50wfMetrwDUXrftb4OCwfBD4mwWY\n87PAXuDkWnMCtwP/CgS4GXhmgWa+H/jLS2x7w/B7cgVw/fD7s20OM+8E9g7LVwKvDLMt9Gs9yWXe\newo3Aaeq6mdV9WvgUWDfnGdaj33A4WH5MHDHHGcBoKqeAt66aPVqc+4DHq6Rp4GrkuyczaQfWGXm\n1ewDHq2qX1XVz4FTjH6PZqqqzlbVj4fld4CXgF0s+Gs9iXlHYRfwxtjt08O6RVTA95McT3JgWLej\nqs4Oy28CO+Yz2ppWm3PRX//7hl3th8YOzRZu5iTXAZ8BnmF5X+vfmncUlsktVbUXuA24N8lnx++s\n0T7iwn+UsyxzAg8CnwZuBM4CX5vvOJeW5BPAd4EvV9Uvx+9bote6mXcUzgC7x25fO6xbOFV1Zrg+\nDzzOaJf13IVdwOH6/Pwm/Eirzbmwr39Vnauq96rqfeCbfHCIsDAzJ/kYoyB8p6q+N6xeutf6YvOO\nwrPAniTXJ7kcuAs4MueZPiTJx5NceWEZ+DxwktGs+4fN9gNPzGfCNa025xHg7uHM+M3A22O7vnN1\n0fH2Fxm93jCa+a4kVyS5HtgD/GgO8wX4FvBSVX197K6le60/ZN5nOhmdlX2F0Vnkr857nlVm/BSj\nM94/AV64MCfwSeAo8CrwA2D7Asz6CKPd7d8wOm69Z7U5GZ0J/8bw2j8PrCzQzP84zHSC0Rtq59j2\nXx1mfhm4bU4z38Lo0OAE8NxwuX3RX+tJLn6jUVIz78MHSQvGKEhqjIKkxihIaoyCpMYoSGqMgqTG\nKEhq/hfG2EdkRRXXFAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7944c15a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[100, 77, 0, :, :], cmap='gray')\n",
    "np.unique(X[100, 77, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splitted_data(X, ages, labels):\n",
    "    assert len(X) == len(labels) == len(ages)\n",
    "    # OneHotEncoding for ages\n",
    "    enc_table = np.eye(AGE_CLASSES)\n",
    "    ages_ohe = np.array([enc_table[int(round(x))] for x in ages])\n",
    "    # Normalize labels\n",
    "    labels /= MAX_SURVIVAL\n",
    "    # Split data into: 70% train, 15% test, 15% validation\n",
    "    cuts = [int(.70*len(X)), int(.85*len(X))]\n",
    "    X1_train, X1_test, X1_val = np.split(X, cuts)\n",
    "    X2_train, X2_test, X2_val = np.split(ages_ohe, cuts)\n",
    "    Y_train, Y_test, Y_val = np.split(labels, cuts)\n",
    "    return X1_train, X2_train, Y_train, X1_test, X2_test, Y_test, X1_val, X2_val, Y_val\n",
    "\n",
    "X1_train, X2_train, Y_train, X1_test, X2_test, Y_test, X1_val, X2_val, Y_val = get_splitted_data(X, df['Age'], df['Survival'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 114 samples, validate on 24 samples\n",
      "Epoch 1/50\n",
      "114/114 [==============================] - 10s - loss: 318350.5620 - acc: 0.0000e+00 - val_loss: 212980.3438 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "114/114 [==============================] - 7s - loss: 318237.8766 - acc: 0.0000e+00 - val_loss: 212980.3438 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " 32/114 [=======>......................] - ETA: 4s - loss: 267642.6875 - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-0f919ed7df26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m               \u001b[0;34m{\u001b[0m\u001b[0;34m'main_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 'aux_output': Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m               validation_data=({'main_input': X1_test, 'aux_input': X2_test}, Y_test))\n\u001b[0m",
      "\u001b[0;32m/home/willi/SegMed/segmed/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/willi/SegMed/segmed/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1143\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                         \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/willi/SegMed/segmed/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_slice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    main_input = Input(shape=X.shape[1:], dtype='float32', name='main_input')\n",
    "    x = Conv3D(16, (3, 3, 3), padding='same', activation='relu')(main_input)\n",
    "    x = Conv3D(16, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling3D(pool_size=(2, 1, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv3D(32, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv3D(32, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling3D(pool_size=(2, 1, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    cnn_out = Dense(64, activation='relu')(x)\n",
    "    # auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(cnn_out)\n",
    "    \n",
    "    auxiliary_input = Input(shape=(AGE_CLASSES,), name='aux_input', dtype='float32')\n",
    "    x = keras.layers.concatenate([cnn_out, auxiliary_input])\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    main_output = Dense(1, activation='sigmoid', name='main_output')(x)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output])  # , auxiliary_output\n",
    "    \n",
    "    # RMSprop uses:\n",
    "    # - Momentum taking knowledge from previous steps into account about where\n",
    "    #   we should be heading (prevents gradient descent to oscillate)\n",
    "    # - Uses recent gradients to adjust alpha\n",
    "    #   (when the gradient is very large, alpha is reduced and vice-versa)\n",
    "    # Later we should test if AdaDelta or Adam are improving our results (quite similar to RMSprop)\n",
    "    model.compile(optimizer='Adam',\n",
    "                  metrics=['accuracy'],\n",
    "                  loss={'main_output': 'mean_squared_error'},  # 'aux_output': 'mean_squared_error'\n",
    "                  loss_weights={'main_output': 1.})  # , 'aux_output': 0.2\n",
    "\n",
    "    # And trained it via:\n",
    "    model.fit({'main_input': X1_train, 'aux_input': X2_train},\n",
    "              {'main_output': Y_train},  # 'aux_output': Y\n",
    "              epochs=50, batch_size=32, verbose=1,\n",
    "              validation_data=({'main_input': X1_test, 'aux_input': X2_test}, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient: Brats17_TCIA_167_1, Age: 74.907, GT: 153, Prediction: 349\n",
      "Patient: Brats17_TCIA_242_1, Age: 66.479, GT: 147, Prediction: 336\n",
      "Patient: Brats17_TCIA_319_1, Age: 64.86, GT: 254, Prediction: 453\n",
      "Patient: Brats17_TCIA_469_1, Age: 63.899, GT: 519, Prediction: 582\n",
      "Patient: Brats17_TCIA_218_1, Age: 57.345, GT: 346, Prediction: 519\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prediction = int(model.predict({'main_input': X[i:i+1], 'aux_input': ages_ohe[i:i+1]}) * MAX_SURVIVAL)\n",
    "    print('Patient: {}, Age: {}, GT: {}, Prediction: {}'.format(df.values[i, 0], df.values[i, 1], df.values[i, 2], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 163 patients with tumor images of sizes 240x240 of which we have 155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start\n",
    "Implementing a concatenated model as shown in the Docs: https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models\n",
    "The model uses a small common CNN for processing one patient (155 slices x 128 width x 128 height) and just inserts the age at a certain point. With this simple model we get really bad results (main_output_loss: 263381.5156 [not getting better]).\n",
    "\n",
    "\n",
    "### Possible improvements\n",
    "1. ~~Conv3D (in CNN) which gets a 4D input - therefore add the tumor region as a dimension (transform input image into three images containing each tumor region seperately~~)\n",
    "2. ~~One-hot encoder (in sklearn or implement manually)\n",
    "  [ I tried Keras Lambda layer but after fixing the shapes keras told me that I'm not allowed to concatenate with a non input layer ]~~\n",
    "3. Group ages into few classes changing the regression task into a classification task (with softmax).\n",
    "4. Evaluate if LSTM is useful for this task (before concatenating the two branches or as a concatenator?)\n",
    "5. Evaluate our optimizer (RMSprp vs Adam vs AdaDelta)\n",
    "    -> RMSprop doesn't learn at all (Adam does!)\n",
    "    -> First run was without normalization but no improvement (After normalization quite good results)\n",
    "6. I red full batch learning is the best way for training on few training data - check if this is helpful.\n",
    "7. Use for a patient nx, ny and nz at the same time\n",
    "8. Data augmentation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
